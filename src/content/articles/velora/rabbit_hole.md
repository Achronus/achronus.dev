---
title: '#2 Down the Rabbit Hole'
slug: down-the-rabbit-hole
date: 31/01/2025
order: 3
---
The LNN paper is hard to read. It requires a lot of preliminary knowledge to even understand what the heck is going on. So, I'm going deeper down the rabbit hole to first understand its root components: *Recurrent Neural Networks* (RNN) and *Neural Circuit Policies* (NCPs; Lechner, Hasani and Grosu, 2018). From what I can initially ascertain, LNNs are just a mathematical expansion of NCPs which are a type of RNN.

It's been a while since I've programmed RNNs so I need to familiarise myself with the basics of them too. Admittedly, I'm biased towards them and have deliberately ignored them over the years, primarily because they use text like LLMs (a story for another day). Their hidden state is pretty remarkable though and extremely useful for that essence of *short-term memory*.

I'll update this article again in a couple of days after I've done some homework! ðŸ˜‰

---



## References

Lechner, M., Hasani, R.M. and Grosu, R. (2018). Neuronal Circuit Policies. [online] arXiv.org. Available at: [https://arxiv.org/abs/1803.08554](https://arxiv.org/abs/1803.08554).
