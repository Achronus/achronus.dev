---
title: '#2 Down the Rabbit Hole'
slug: down-the-rabbit-hole
date: 31/01/2025
order: 3
---
The LNN paper is hard to read. It requires a lot of preliminary knowledge to even understand what the heck is going on. So, I'm going deeper down the rabbit hole to first understand its root components: *Recurrent Neural Networks* (RNN) and *Neural Circuit Policies* (NCPs; Lechner, Hasani and Grosu, 2018). From what I can initially ascertain, LNNs are just a mathematical expansion of NCPs which are a type of RNN.

It's been a while since I've programmed RNNs so I need to familiarise myself with the basics of them too. Admittedly, I'm biased towards them and have deliberately ignored them over the years, primarily because they use text like LLMs (a story for another day). Their hidden state is pretty remarkable though and extremely useful for that essence of *short-term memory*.

I'll update this article again in a couple of days after I've done some homework! ðŸ˜‰

---

It's been 3 days since I started this journal entry and I've learned quite a bit but still have more to go! I now have a better grasp on LNNs and know that they are based on a custom RNN cell called Liquid-Time Constants (LTCs). They allow networks to learn the dynamics of a system and can be drastically scaled.

What's interesting is, LTCs add more expressiveness to RNN models but the NCP is the small sparsely connected neuron architecture. That's the next piece of the puzzle I need to explore.
I'm putting all of the stuff I've learned into a Medium article as we speak, so keep an eye out for that! Once that's done, I can start planning Velora out properly.

We'll discuss that in the next entry. See you there! ðŸ‘‹

## References

Lechner, M., Hasani, R.M. and Grosu, R. (2018). Neuronal Circuit Policies. [online] arXiv.org. Available at: [https://arxiv.org/abs/1803.08554](https://arxiv.org/abs/1803.08554).
